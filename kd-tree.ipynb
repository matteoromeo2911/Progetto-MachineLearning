{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importare le librerie necessarie\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KDTree\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importare le librerie necessarie\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "\n",
    "# 1. Caricamento del dataset\n",
    "file_path = 'Data/features_3_sec.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Stampa i nomi delle colonne per verificare\n",
    "# print(\"Colonne nel DataFrame:\", data.columns)\n",
    "\n",
    "# 2. Separare le caratteristiche (X) e le etichette (y)\n",
    "X = data.drop(columns=['filename', 'label'])  # Rimuovi filename e label\n",
    "y = data['label']  # Etichette\n",
    "\n",
    "# 3. Normalizzazione delle caratteristiche\n",
    "# Standardizzare le caratteristiche\n",
    "robust = RobustScaler()\n",
    "standard = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "power = PowerTransformer()\n",
    "scalers=[robust, standard, minmax, power]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> STANDARD KD-TREES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Scaler: RobustScaler()\n",
      "Accuratezza: 0.88\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1-score: 0.88\n",
      "F2-score: 0.88\n",
      "------------------------------------------------------------\n",
      "           Scaler: StandardScaler()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n",
      "           Scaler: MinMaxScaler()\n",
      "Accuratezza: 0.91\n",
      "Precision: 0.91\n",
      "Recall: 0.91\n",
      "F1-score: 0.91\n",
      "F2-score: 0.91\n",
      "------------------------------------------------------------\n",
      "           Scaler: PowerTransformer()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for scaler in scalers:\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 4. Suddivisione del dataset in training e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 5. Creazione del KD-Tree con i dati di addestramento\n",
    "    kd_tree = KDTree(X_train, leaf_size=30)\n",
    "\n",
    "    # 6. Funzione per trovare i vicini pi첫 prossimi\n",
    "    def knn_with_kdtree(kd_tree, X_test, X_train, y_train, k=5):\n",
    "        # Per ogni punto del test set, trova i k vicini pi첫 prossimi\n",
    "        distances, indices = kd_tree.query(X_test, k=k)\n",
    "        \n",
    "        # Predici il genere musicale basato sui vicini\n",
    "        predictions = []\n",
    "        for neighbors in indices:\n",
    "            # Trova la classe maggioritaria tra i vicini\n",
    "            neighbor_labels = y_train.iloc[neighbors]\n",
    "            predicted_label = neighbor_labels.mode()[0]\n",
    "            predictions.append(predicted_label)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    # 7. Usare il KD-Tree per classificare i dati di test\n",
    "    y_pred = knn_with_kdtree(kd_tree, X_test, X_train, y_train, k=5)\n",
    "\n",
    "     # Calcolare l'accuratezza\n",
    "    # Calcolare metriche\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # Macro per bilanciare le classi\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')  # F2-score con beta=2\n",
    "\n",
    "    # Stampare i risultati\n",
    "    print(f'           Scaler: {scaler}')\n",
    "    print(f'Accuratezza: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'F1-score: {f1:.2f}')\n",
    "    print(f'F2-score: {f2:.2f}')\n",
    "   # print(f'Hinge Loss: {loss}')\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> BALANCED KD-TREES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Scaler: RobustScaler()\n",
      "Accuratezza: 0.88\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1-score: 0.88\n",
      "F2-score: 0.88\n",
      "------------------------------------------------------------\n",
      "           Scaler: StandardScaler()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n",
      "           Scaler: MinMaxScaler()\n",
      "Accuratezza: 0.91\n",
      "Precision: 0.91\n",
      "Recall: 0.91\n",
      "F1-score: 0.91\n",
      "F2-score: 0.91\n",
      "------------------------------------------------------------\n",
      "           Scaler: PowerTransformer()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for scaler in scalers:\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 4. Suddivisione del dataset in training e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 5. Creazione del KD-Tree con i dati di addestramento\n",
    "    kd_tree = KDTree(X_train, leaf_size=30)\n",
    "\n",
    "    # 6. Funzione per trovare i vicini pi첫 prossimi\n",
    "    def knn_with_balanced_kdtree(kd_tree, X_test, X_train, y_train, k=5):\n",
    "        # Per ogni punto del test set, trova i k vicini pi첫 prossimi\n",
    "        distances, indices = kd_tree.query(X_test, k=k)\n",
    "        \n",
    "        # Predici il genere musicale basato sui vicini\n",
    "        predictions = []\n",
    "        for neighbors in indices:\n",
    "            # Trova la classe maggioritaria tra i vicini\n",
    "            neighbor_labels = y_train.iloc[neighbors]\n",
    "            predicted_label = neighbor_labels.mode()[0]\n",
    "            predictions.append(predicted_label)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    # 7. Usare il KD-Tree per classificare i dati di test\n",
    "    y_pred = knn_with_balanced_kdtree(kd_tree, X_test, X_train, y_train, k=5)\n",
    "\n",
    "     # Calcolare l'accuratezza\n",
    "    # Calcolare metriche\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # Macro per bilanciare le classi\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')  # F2-score con beta=2\n",
    "\n",
    "    # Stampare i risultati\n",
    "    print(f'           Scaler: {scaler}')\n",
    "    print(f'Accuratezza: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'F1-score: {f1:.2f}')\n",
    "    print(f'F2-score: {f2:.2f}')\n",
    "   # print(f'Hinge Loss: {loss}')\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> APPROXIMATE KD-TREES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Scaler: RobustScaler()\n",
      "Accuratezza: 0.88\n",
      "Precision: 0.88\n",
      "Recall: 0.88\n",
      "F1-score: 0.88\n",
      "F2-score: 0.88\n",
      "------------------------------------------------------------\n",
      "           Scaler: StandardScaler()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n",
      "           Scaler: MinMaxScaler()\n",
      "Accuratezza: 0.91\n",
      "Precision: 0.91\n",
      "Recall: 0.91\n",
      "F1-score: 0.91\n",
      "F2-score: 0.91\n",
      "------------------------------------------------------------\n",
      "           Scaler: PowerTransformer()\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for scaler in scalers:\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 4. Suddivisione del dataset in training e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 5. Creazione del KD-Tree con i dati di addestramento\n",
    "    kd_tree = KDTree(X_train, leaf_size=30, metric='euclidean')\n",
    "\n",
    "    # 6. Funzione per trovare i vicini pi첫 prossimi\n",
    "    def knn_with_approx_kdtree(kd_tree, X_test, X_train, y_train, k=5):\n",
    "        # Per ogni punto del test set, trova i k vicini pi첫 prossimi\n",
    "        distances, indices = kd_tree.query(X_test, k=k)\n",
    "        \n",
    "        # Predici il genere musicale basato sui vicini\n",
    "        predictions = []\n",
    "        for neighbors in indices:\n",
    "            # Trova la classe maggioritaria tra i vicini\n",
    "            neighbor_labels = y_train.iloc[neighbors]\n",
    "            predicted_label = neighbor_labels.mode()[0]\n",
    "            predictions.append(predicted_label)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    # 7. Usare il KD-Tree per classificare i dati di test\n",
    "    y_pred =knn_with_approx_kdtree(kd_tree, X_test, X_train, y_train, k=5)\n",
    "\n",
    "     # Calcolare l'accuratezza\n",
    "    # Calcolare metriche\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # Macro per bilanciare le classi\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')  # F2-score con beta=2\n",
    "\n",
    "    # Stampare i risultati\n",
    "    print(f'           Scaler: {scaler}')\n",
    "    print(f'Accuratezza: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'F1-score: {f1:.2f}')\n",
    "    print(f'F2-score: {f2:.2f}')\n",
    "   # print(f'Hinge Loss: {loss}')\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> DYAMIC KD-TREES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler: StandardScaler\n",
      "Accuratezza: 0.92\n",
      "Precision: 0.92\n",
      "Recall: 0.92\n",
      "F1-score: 0.92\n",
      "F2-score: 0.92\n",
      "------------------------------------------------------------\n",
      "Scaler: MinMaxScaler\n",
      "Accuratezza: 0.93\n",
      "Precision: 0.93\n",
      "Recall: 0.93\n",
      "F1-score: 0.93\n",
      "F2-score: 0.93\n",
      "------------------------------------------------------------\n",
      "Scaler: RobustScaler\n",
      "Accuratezza: 0.90\n",
      "Precision: 0.90\n",
      "Recall: 0.90\n",
      "F1-score: 0.90\n",
      "F2-score: 0.90\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "\n",
    "class StratifiedKDTree:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Seleziona l'asse da dividere\n",
    "        k = X.shape[1]  # Numero di dimensioni\n",
    "        axis = depth % k\n",
    "        \n",
    "        # Ordina i punti in base all'asse selezionato\n",
    "        sorted_indices = np.argsort(X[:, axis])\n",
    "        X_sorted = X[sorted_indices]\n",
    "        y_sorted = y.iloc[sorted_indices]\n",
    "\n",
    "        # Calcola il punto medio\n",
    "        median_index = len(y_sorted) // 2\n",
    "        \n",
    "        # Crea il nodo e costruisci i sotto-alberi\n",
    "        return {\n",
    "            'point': X_sorted[median_index],\n",
    "            'label': y_sorted.iloc[median_index],\n",
    "            'left': self.build_tree(X_sorted[:median_index], y_sorted[:median_index], depth + 1),\n",
    "            'right': self.build_tree(X_sorted[median_index + 1:], y_sorted[median_index + 1:], depth + 1)\n",
    "        }\n",
    "\n",
    "    def query(self, point):\n",
    "        return self._query(self.tree, point)\n",
    "\n",
    "    def _query(self, node, point, depth=0):\n",
    "        if node is None:\n",
    "            return None\n",
    "\n",
    "        # Calcola la distanza dall'attuale punto\n",
    "        k = len(point)\n",
    "        axis = depth % k\n",
    "        \n",
    "        next_branch = None\n",
    "        opposite_branch = None\n",
    "        \n",
    "        # Controlla quale ramo esplorare\n",
    "        if point[axis] < node['point'][axis]:\n",
    "            next_branch = node['left']\n",
    "            opposite_branch = node['right']\n",
    "        else:\n",
    "            next_branch = node['right']\n",
    "            opposite_branch = node['left']\n",
    "\n",
    "        # Ricerca nel ramo successivo\n",
    "        best = self._query(next_branch, point, depth + 1)\n",
    "\n",
    "        # Confronta con il nodo attuale\n",
    "        if best is None or np.linalg.norm(point - node['point']) < np.linalg.norm(point - best['point']):\n",
    "            best = node\n",
    "\n",
    "        # Controlla se dobbiamo esplorare l'altro ramo\n",
    "        if abs(point[axis] - node['point'][axis]) < np.linalg.norm(point - best['point']):\n",
    "            candidate = self._query(opposite_branch, point, depth + 1)\n",
    "            if candidate is not None and np.linalg.norm(point - candidate['point']) < np.linalg.norm(point - best['point']):\n",
    "                best = candidate\n",
    "\n",
    "        return best\n",
    "\n",
    "# Carica il tuo dataset\n",
    "file_path = 'Data/features_3_sec.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separare le caratteristiche e le etichette\n",
    "X = data.drop(columns=['filename', 'label']).values\n",
    "y = data['label']\n",
    "\n",
    "# Definire i vari scaler\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]\n",
    "\n",
    "# Per ogni scaler, normalizzare i dati e costruire il KD-tree stratificato\n",
    "for scaler in scalers:\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Suddivisione del dataset in training e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Costruzione del KD-tree stratificato\n",
    "    kd_tree = StratifiedKDTree(X_train, y_train)\n",
    "\n",
    "    # Predizioni\n",
    "    y_pred = []\n",
    "    for point in X_test:\n",
    "        best = kd_tree.query(point)\n",
    "        y_pred.append(best['label'])\n",
    "\n",
    "    # Calcolare metriche\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')  # Macro per bilanciare le classi\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')  # F2-score con beta=2\n",
    "\n",
    "    # Stampare i risultati\n",
    "    print(f'Scaler: {scaler.__class__.__name__}')\n",
    "    print(f'Accuratezza: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'F1-score: {f1:.2f}')\n",
    "    print(f'F2-score: {f2:.2f}')\n",
    "    print('------------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
